---
title: "STAT 571 - HW4"
author:
- Aditi Jayashankar
- Eddie Kong
- Sahana Vijaya Prasad
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=5, fig.width=11, warning = F)

# Load all packages
if(!require('pacman')) {
  install.packages('pacman')}
pacman::p_load( ISLR, tidyverse, tree, randomForest, rpart, partykit, gbm, car, tm, SnowballC, RTextTools, RColorBrewer, wordcloud, glmnet)
```

## Problem 1
Conceptual problem to get familiar with the building blocks for decision trees.

#### (1)
A small data set is generated and it is stored in data2. It consists of four variables 

Y1: a continuous response
Y2: a binary response and 
X1, X2 two continuous explanatory variables.


```{r}
set.seed(1)
x.temp <- ceiling(runif(40, min=0, max=100))
data1 <- matrix(x.temp,ncol=2, byrow=TRUE )
y <- round(rexp(nrow(data1), rate=2), 2)
data1 <- data.frame(data1, y)
names(data1)  <- c("X1", "X2", "Y1")

data2 <- data1
set.seed(1)
data2$Y2 <- ifelse((data1$X1+data1$X2 > 70), rbinom(1,1,.62), rbinom(1,1, .31))
data2
```

#### (2)
A diagram is drawn to partition X1 and X2 into R1, . R6 regions

```{r}
# Set up an empty plot
plot(NA, NA, type = "n", xlim = c(0,100), ylim = c(0,100), xlab = "X1", ylab = "X2", pch=16)
# Draw some horizontal and vertical lines to divide the space into 6 regions
lines(x = c(40,40), y = c(0,100))
lines(x = c(0,40), y = c(75,75))
lines(x = c(75,75), y = c(0,100))
lines(x = c(20,20), y = c(0,75))
lines(x = c(75,100), y = c(25,25))

# Label the regions
text(x = (40+75)/2, y = 50, labels = c("R1"))
text(x = 20, y = (100+75)/2, labels = c("R2"))
text(x = (75+100)/2, y = (100+25)/2, labels = c("R3"))
text(x = (75+100)/2, y = 25/2, labels = c("R4"))
text(x = 30, y = 75/2, labels = c("R5"))
text(x = 10, y = 75/2, labels = c("R6"))

points(data2$X1, data2$X2, pch=16, col='red')
```


##### (i) Is this a top-down, recursive tree? 
Yes, because once we split we never go back, we keep going down.

Use this tree with data2 as the training data. Give the following predicted values of Y1 on the end nodes using X1 and X2:
##### (ii) Predicted Y1 for x1=60, x2=30. 

This falls under Region 1. Adding the Y1 values of all the 8 points in R1 and taking the average,

```{r}
R1 <- c(0.15, 0.03, 0.198, 0.5, 0.16, 0.15, 0.36, 0.54)
pred.1 <- sum(R1)/length(R1)
```
The predicted Y1 for x1=60, x2=30 is `r pred.1`

##### (iii) Predicted Y1 for x1=90, x2=10.

This falls under Region 4. Adding the Y1 values of the two points and taking the average,
```{r}
R4 <- c(0.12, 0.02)
pred.2 <- sum(R4)/length(R4)
```
The predicted Y1 for x1=90, x2=10 is `r pred.2`

#### (3) 
Use tree() to produce a best decision tree for Y1. Display the tree. Is this tree very different from the decision tree given in the diagram above? 

```{r}
tree.fit <- tree(Y1~X1+X2, data2)
plot(tree.fit)
text(tree.fit, pretty=0) 
```

Yes, the tree is different from the decision tree given in the diagram above. This tree only uses X2 and has 3 terminal nodes. The previous tree used both X1 and X2, and had 6 terminal nodes.

#### (4) 
Let us now concentrate on classification decision trees. The event of interests is Y2=1. 

##### (i) Overlay the labels Y2 for each subject in the original tree of the appendix. 

```{r}
# Set up an empty plot
plot(NA, NA, type = "n", xlim = c(0,100), ylim = c(0,100), xlab = "X1", ylab = "X2", pch=16, 
   main = "Y2=0 in Red, Y2=1 in Blue")
# Draw some horizontal and vertical lines to divide the space into 6 regions
lines(x = c(40,40), y = c(0,100))
lines(x = c(0,40), y = c(75,75))
lines(x = c(75,75), y = c(0,100))
lines(x = c(20,20), y = c(0,75))
lines(x = c(75,100), y = c(25,25))

# Label the regions
text(x = (40+75)/2, y = 50, labels = c("R1"))
text(x = 20, y = (100+75)/2, labels = c("R2"))
text(x = (75+100)/2, y = (100+25)/2, labels = c("R3"))
text(x = (75+100)/2, y = 25/2, labels = c("R4"))
text(x = 30, y = 75/2, labels = c("R5"))
text(x = 10, y = 75/2, labels = c("R6"))
points(data2$X1[data2$Y2 == 0], data2$X2[data2$Y2 == 0], pch=16, col='red')
points(data2$X1[data2$Y2 == 1], data2$X2[data2$Y2 == 1], pch=16, col='blue')
#points(data2$X1, data2$X2, pch=16, col='red')
text(data2$X1, data2$X2+4, labels = data2$Y2)
```

Data2 will be again the training data and we use sample proportion to estimate the probability of Y2=1 in each region. 
##### (ii) Predicted Prob(Y2=1) for x1=60, x2=30

This falls under Region 1. There are 8 points in R1. 6 of them have Y2 = 1 and 2 have Y2 = 0.
Therefore the Prob(Y2=1) for x1=60, x2=30 is 6/8.

##### (iii) Give Y2's label for x1=60, x2=30 by majority vote. 

Since 6 out of the 8 points in Region 1 have Y2 = 1, by majority vote, label for x1=60, x2=30 is Y2 = 1

#### (5)
Apply rpart() by default to produce a decision classification tree and plot it. Is this tree different from our original tree? 


```{r}
tree.fit.2 <- rpart(Y2~X1+X2, data2)

plot(as.party(tree.fit.2))
```

Yes, this tree is different from our original tree. This tree only uses X1 to form the decision tree, and has 3 terminal nodes. Our original tree used both X1 and X2, and had 6 terminal nodes.


## Problem 2
Exercise designed to understand PCA conceptually.

```{r}
#load data set
iq_data <- read.csv("IQ.Full.csv")
set.seed(10)

#take subset of 100 subjects
sample100 <- iq_data[sample(1:nrow(iq_data), 100,
  	replace=FALSE),c("Coding","Auto","Mechanic","Elec","Science","Math", "Arith", "Word", "Parag", "Numer")] 
```

### (1)
Let us first use PCA to summarize the ASVAB tests. Run prcomp over all the tests in ASVAB. We should center and scale all the tests.

```{r}
pc.asvab <- prcomp(sample100, scale=TRUE)
```


#### (a) Report the PC1 and PC2 loadings. Are they unit vectors? Are they uncorrelated? 
```{r}
pc1.loadings <- data.frame(pc.asvab$rotation[,"PC1"])
pc2.loadings <- data.frame(pc.asvab$rotation[,"PC2"])
cor(pc1.loadings, pc2.loadings)
```

PC1 and PC2 are uncorrelated in vector space. Their correlation value is 0.159.

```{r}
sum(pc1.loadings^2)
sum(pc2.loadings^2)
```

As the squares of the laodings add up to 1, they are unit vectors.

The loadings for PC1 are:
```{r}
pc1.loadings
```

The loadings for PC2 are:
```{r}
pc2.loadings
```

#### (b) How is the PC1 score obtained for each subject? Write down the correction.

The PC1 score is obtained by substituting the PC1 loadings and each person's ASVAB scores into the following equation. $$Z_1 = \phi_{11}*X_1 + \phi_{12}*X_2 +  ... \phi_{19}*X_9 + \phi_{110}*X_10$$

 More concretely, it is Z1 = `r pc1.loadings[,1][1]` * X1 + 
 `r pc1.loadings[,1][2]` * X2 + 
 `r pc1.loadings[,1][3]` * X3 + 
 `r pc1.loadings[,1][4]` * X4 + 
 `r pc1.loadings[,1][5]` * X5 + 
 `r pc1.loadings[,1][6]` * X6 + 
 `r pc1.loadings[,1][7]` * X7 + 
 `r pc1.loadings[,1][8]` * X8 + 
 `r pc1.loadings[,1][9]` * X9 + 
 `r pc1.loadings[,1][10]` * X10 
 
#### (c) Are PC1 scores and PC2 scores in the data uncorrelated? 

```{r}
pc1.scores <- data.frame(pc.asvab$x[,"PC1"])
pc2.scores <- data.frame(pc.asvab$x[,"PC2"])
cor(pc1.scores, pc2.scores)
```

Yes, PC1 and PC2 scores are uncorrelated. We can verify that from the small value obtained above.

#### (d) Plot PVE (Proportion of Variance Explained) with an explanation.

```{r}
pve.asvab <- 100* (pc.asvab$sdev)^2/sum ((pc.asvab$sdev)^2)
plot(pve.asvab, pch=16, 
     xlab="Principal Components",
     ylab="Prop. of variance explained",
     main="PVE ASVAB plot")
```

This graph shows the proportion of the total variance that each principal component explains. The first component explains `r pve.asvab[1]`% of the total variance, the second PC component explains `r pve.asvab[2]`% of the total variance and so on.

#### (e) Also plot CPVE (Cumulative Proportion of Variance Explained). What proportion of the variance in the data is explained by the first two principal components?
```{r}
cpve.asvab <- 100*cumsum((pc.asvab$sdev)^2)/10   

# Scree plot of CPVE's
plot(seq(1:10), cpve.asvab, pch=16, ylim=c(0, 100),
     main="Cumulative Proportion of Variance Explained",
     xlab="Number of PC's Used")

first2pcvariance <- cpve.asvab[2]
```
The first two components explain `r first2pcvariance`% of the total variance.

#### (f) PC's provide us with a low dimensional view of the ASVAB. Use a biplot to display the data, using the first two principal components. Give an interpretation from the plot.


```{r}
lim <- c(-0.3, 0.3) 
biplot(pc.asvab, xlim=lim,
       ylim=lim,
       main="Biplot of the PC's")
abline(v=0, h=0)
```
Here we plot PC1 as the x-axis and PC2 as the y-axis. We can see that the graph has an overall flattened oval shape which closely matches what we expect. PC1 has higher variability and so we expect its "spread" to be greater than PC2's. 

The angle between the vectors indicates the correlation between them. From the plot we can see that Word and Math scores are highly correlated. The five scores: Numer, Parag, Word, Math and Arith vectors are close to eachother, which shows that there is high correlation between them. The points that are furthest in the left corner are those with the highest scores in all the areas, while the ones in the right are those with the lowest scores. The points which are in the top quadrants are those who scored higher in Coding, Word, Parag, Numer, Math compared to the other scores, while the ones in the bottom quadrants are those who scored higher in Auto, Mechanic, Elec and Science.


#### (g) Repeat the above biplot but label points with different colors, according to their Gender. Do you see a systematic separation between Male and Female in the biplot? Write a brief summary about your findings.

```{r}
plot(pc.asvab$x[, 1], pc.asvab$x[, 2], col=iq_data$Gender,
     xlim=c(-10, 10), ylim=c(-10, 10),
     xlab="PC1", ylab="PC2")
abline(v=0, h=0)
legend("bottomright", legend=c(as.character(levels(iq_data$Gender))),
       lty=c(1,1), lwd=c(2,2), col=iq_data$Gender)
```

There seems to be no systematic separation between Female and Male using just these two principal components.


### (2) 
We next will try to summarize the 10 Esteem measurement by PCA

#### (a) First, notice that Esteem 1, 2, 4, 6, and 7 need to be reversed prior to scoring in order for a higher score to designate higher self-esteem. 

```{r}
set.seed(10)

#take subset of 100 subjects
esteem100 <- iq_data[sample(1:nrow(iq_data), 100,
  	replace=FALSE),c("Esteem1", "Esteem2", "Esteem3", "Esteem4", "Esteem5", "Esteem6", "Esteem7", "Esteem8", "Esteem9", "Esteem10")]

#reverse esteem scores
esteem100[,  c(1, 2, 4, 6, 7)]  <- 5- esteem100[,  c(1, 2, 4, 6, 7)]
```

#### (b) What are the PC1 loadings? 

```{r}
#esteem100.scale <- scale(esteem100, center=TRUE, scale=TRUE)
pc.esteem <- prcomp(esteem100, scale=TRUE)
pc1.esteem.loadings <- data.frame(pc.esteem$rotation[,"PC1"])
```
The loadings for PC1 are
```{r}
pc1.esteem.loadings
```

#### (c) How much variance is explained by using the PC1? Provide both PVE and CPVE plots

```{r}
# PVE plot
pve.esteem <- 100* (pc.esteem$sdev)^2/sum ((pc.esteem$sdev)^2)
plot(pve.esteem, pch=16, 
     xlab="Principal Components",
     ylab="Prop. of variance explained",
     main="PVE esteem plot")
```

```{r}
cpve.esteem <- 100*cumsum((pc.esteem$sdev)^2)/10   

# Scree plot of CPVE's
plot(seq(1:10), cpve.esteem, pch=16, ylim=c(0, 100),
     main="Cumulative Proportion of Variance Explained",
     xlab="Number of PC's Used")

```
PC1 explains `r pve.esteem[1]`% of the total variance.

#### (d) Combine c) and the biplot of the PC1 and PC2 write a brief summary about Esteem 


```{r}
lim <- c(-0.3, 0.3) 
biplot(pc.esteem, xlim=lim,
       ylim=lim,
       main="Biplot of the esteem PC's")
abline(v=0, h=0)
```
Some of the Esteem scores seem highly correlated with one another. Esteem2 and Esteem4 scores are highly correlated with eachother, while Esteem3, Esteem5, Esteem6 and Esteem7 scores are highly correlated with eachother. The points which are on the left side score higher among all the Esteem scores while the ones on the right score lower.

### (3)
How well can we predict 'success' based on Intelligence? 
To answer this question, we use Income <- log(Income2005) as a measure of one's success.

#### (a) Why is it important to create a logarithmic transformation of income?

It is important to use the logarithmic transformation for income because a unit increase at high amounts of income means less than a unit increase at a low income (poor student vs wealthy professor example from class where 10$ means more to the student than professor). Adjusting it to a logarithmic scale adjusts for the nature appropriately.

#### (b) Run prcomp over ASVAB tests first.

```{r}
pc.asvab.all <- prcomp(iq_data[,c("Coding","Auto","Mechanic","Elec","Science","Math", "Arith", "Word", "Parag", "Numer")], scale=TRUE)
```


#### (c) fit1: Income ~ PC1; fit2: Income ~ PC1+PC2+PC3. Notice the LS estimates of PC1 in both fit1 and fit2 are identical. Why is this so? Are the leading PC's of ASVAB significant variables to predict Income? 

```{r}
logIncome <- log(iq_data$Income2005)
data.b.pc <- data.frame(logIncome, pc.asvab.all$x)
fit1 <- lm(logIncome~PC1, data.b.pc)

summary(fit1)
```

```{r}
fit2 <- lm(logIncome~PC1+PC2+PC3+PC4+PC5, data.b.pc)
summary(fit2)
```


If variables are uncorrelated, the LS estimates are the same in both simple and multiple linear regression. As all the PC values are uncorrelated, the LS estimates of PC1 in both fit1 and fit2 are identical.

```{r}
cpve.asvab.all <- 100*cumsum((pc.asvab.all$sdev)^2)/10  

# Scree plot of CPVE's
plot(seq(1:10), cpve.asvab.all, pch=16, ylim=c(0, 100),
     main="Cumulative Proportion of Variance Explained",
     xlab="Number of PC's Used")
```

Using the elbow rule on the CPVE plot, we take the first 5 leading PCs. The 5 leading PCs are significant at the 0.05 level for predicting Income. However, PC1, PC2, PC4 and PC5 are significant at the 0.001 level.


#### (d) Controlling for Personal Demographic Variables and Household Environment, are the leading PC's of ASVAB significant variables to predict Income at .01 level? Give a brief summary of your findings. 

```{r}
data.b.pc.control <- data.frame(logIncome, pc.asvab.all$x, iq_data[,c("Race","Gender","Educ")], iq_data[,c("Imagazine","Inewspaper","Ilibrary","MotherEd","FatherEd")])

fit3 <- lm(logIncome~PC1+PC2+PC3+PC4+PC5+Imagazine+Inewspaper+Ilibrary+MotherEd+FatherEd+Race+Gender+Educ, data.b.pc.control)

Anova(fit3)
```
Controlling for personal demographic variables and household environment variables, the leading 5 PCs are not significant at the 0.01 level for predicting income. However, they are significant at 0.05 level.

## Problem 3 Case study: Yelp review 

### (1) 

Take a random sample of 20000 reviews (set.seed(1)) from our original data set. Extract document term matrix for texts to keep words appearing at least 2% of the time among all 20000 documents. Go through the similar process of cleansing as we did in the lecture. 

```{r}
yelp.data <- read.csv("yelp_subset.csv", as.is=TRUE)

set.seed(1)

# random sample of 20,000 reviews
yelp.sample <- yelp.data[sample(1:nrow(yelp.data), 20000,
  	replace=FALSE),]

# 1. Make corpus which is collection of texts
corpus1 <- Corpus(VectorSource(yelp.sample$text))

# 2, Change to lower case
corpus2 <- tm_map(corpus1, content_transformer(tolower))

# 3, Remove some non-content words 
corpus3<- tm_map(corpus2, removeWords, stopwords("english"))

# 4, Remove punctuations
corpus4 <- tm_map(corpus3, removePunctuation)

# 5, Remove numbers 
corpus5 <- tm_map(corpus4, removeNumbers)

# 6, Stem words
corpus6 <- tm_map(corpus5, stemDocument) # Removing lazy = TRUE according to Piazza post 233

# 7, Ready to get word frequency matrix
dtm <- DocumentTermMatrix(corpus6)   ## library = collection of words for all documents

# Cut the bag to only include the words appearing at least 2% of the time
threshold <- .02*length(corpus6)   # 2% of the total documents 
words.20 <- findFreqTerms(dtm, lowfreq=threshold)  # words appearing at least among 2% of the documents

dtm.20 <- DocumentTermMatrix(corpus6, control=list(dictionary = words.20))
inspect(dtm.20[100, 405])
```

#### (i) Briefly explain what does this matrix record? What is the cell number at row 100 and column 405? What does it represent?

This matrix records the frequency of words per document. The 100th row represents the 100th document. The 405th column represents the 405th word which is `wall`. Together, the 100, 405 tells you the frequency with which the 405th word appears in the 100th document. Its value is 0 indicating that the word wall doesn't appear in the 100th document.

#### (ii) What is the sparsity of the dtm obtained here? What does that mean?

The sparsity of the dtm is 100%. This means that the matrix is extremely sparse and that there are many 0's across all entries in the matrix. This makes sense as every document only contains a small set of words from the whole bag of words.

### (2) 

Set the stars as a two category response variable called rating to be "1" = 5,4 and "0"= 1,2,3. Combine the variable rating with the dtm as a data frame called data2. Get a training data with 15000 reviews and the rest 5000 reserved as the testing data. 

```{r}
# Set stars as a categorical variable. Also group stars 1-3 as 0 and 4,5 as 1
yelp.sample$rating <- c(0)
yelp.sample$rating[yelp.sample$stars >= 4] <- 1
yelp.sample$rating <- as.factor(yelp.sample$rating)

#combine dtm and rating
data2 <- data.frame(yelp.sample,as.matrix(dtm.20) )  

set.seed(19)
#get training and testing split
train <- sample(1:nrow(yelp.sample), 15000, replace=FALSE)
yelp.train <- data2[train, -c(1:10)] # only keep rating and the texts
yelp.test <- data2[-(train), -c(1:10)]
```

### (3)

Use the training data to get Lasso fit. Choose lambda.1se. Keep the result here.

```{r}
Y.lasso <- yelp.train$rating
X.lasso <- as.matrix(yelp.train[, -c(1)]) # we can use as.matrix directly here

set.seed(2)

result.lasso <- cv.glmnet(X.lasso, Y.lasso, alpha=.99, family="binomial") 
plot(result.lasso)
```
```{r}
beta.lasso <- coef(result.lasso, s="lambda.1se")   # output lasso estimates
beta <- beta.lasso[which(beta.lasso !=0),] # non zero beta's
beta <- as.matrix(beta);
beta <- rownames(beta)
beta
```

### (4)
Feed the output from Lasso above, get a logistic regression

#### (i) Pull out all the positive coefficients and the corresponding words. Rank the coefficients in a decreasing order. Report the leading 2 words and the coefficients. Describe briefly the interpretation for those two coefficients. 

#### (ii) Make a word cloud with the top 100 positive words according to their coefficients. Interpret the cloud briefly.

#### (iii) Repeat i) and ii) for the bag of negative words.

#### (iv) Summarize the findings. 

### (5)
  

