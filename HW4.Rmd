---
title: "STAT 571 - HW4"
author:
- Aditi Jayashankar
- Eddie Kong
- Sahana Vijaya Prasad
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load all packages
if(!require('pacman')) {
  install.packages('pacman')}
pacman::p_load( ISLR, tidyverse, tree, randomForest, rpart, partykit, gbm, car, tm, SnowballC, RTextTools, RColorBrewer, wordcloud, glmnet)
```

## Problem 1

#### 1)

```{r}
set.seed(1)
x.temp <- ceiling(runif(40, min=0, max=100))
data1<- matrix(x.temp,ncol=2, byrow=TRUE )
y <- round(rexp(nrow(data1), rate=2), 2)
data1 <- data.frame(data1, y)
names(data1)  <- c("X1", "X2", "Y1")

data2 <- data1
set.seed(1)
data2$Y2 <- ifelse((data1$X1+data1$X2 > 70), rbinom(1,1,.62), rbinom(1,1, .31))
data2
```

#### 2)

```{r}
# Set up an empty plot
plot(NA, NA, type = "n", xlim = c(0,100), ylim = c(0,100), xlab = "X1", ylab = "X2", pch=16)
# Draw some horizontal and vertical lines to divide the space into 6 regions
lines(x = c(40,40), y = c(0,100))
lines(x = c(0,40), y = c(75,75))
lines(x = c(75,75), y = c(0,100))
lines(x = c(20,20), y = c(0,75))
lines(x = c(75,100), y = c(25,25))

# Label the regions
text(x = (40+75)/2, y = 50, labels = c("R1"))
text(x = 20, y = (100+75)/2, labels = c("R2"))
text(x = (75+100)/2, y = (100+25)/2, labels = c("R3"))
text(x = (75+100)/2, y = 25/2, labels = c("R4"))
text(x = 30, y = 75/2, labels = c("R5"))
text(x = 10, y = 75/2, labels = c("R6"))

points(data2$X1, data2$X2, pch=16, col='red')
```
##### i) 
Yes.

##### ii)
R1

##### iii)
R4

#### 3)

```{r}
tree.fit <- tree(Y1~X1+X2, data2)
plot(tree.fit)
text(tree.fit, pretty=0) 
```

Yes. The tree is different. This tree only uses X2 and has 3 nodes. The previous tree used X1 and X2 and had 6 nodes.

#### 4)

##### i)

```{r}
# Set up an empty plot
plot(NA, NA, type = "n", xlim = c(0,100), ylim = c(0,100), xlab = "X1", ylab = "X2", pch=16, 
   main = "Y2=0 in Red, Y2=1 in Blue")
# Draw some horizontal and vertical lines to divide the space into 6 regions
lines(x = c(40,40), y = c(0,100))
lines(x = c(0,40), y = c(75,75))
lines(x = c(75,75), y = c(0,100))
lines(x = c(20,20), y = c(0,75))
lines(x = c(75,100), y = c(25,25))

# Label the regions
text(x = (40+75)/2, y = 50, labels = c("R1"))
text(x = 20, y = (100+75)/2, labels = c("R2"))
text(x = (75+100)/2, y = (100+25)/2, labels = c("R3"))
text(x = (75+100)/2, y = 25/2, labels = c("R4"))
text(x = 30, y = 75/2, labels = c("R5"))
text(x = 10, y = 75/2, labels = c("R6"))

points(data2$X1[data2$Y2 == 0], data2$X2[data2$Y2 == 0], pch=16, col='red')
points(data2$X1[data2$Y2 == 1], data2$X2[data2$Y2 == 1], pch=16, col='blue')
```

##### ii)

6/8

##### iii)

Y2 = 1

#### 5)

```{r}
tree.fit.2 <- rpart(Y2~X1+X2, data2)

plot(as.party(tree.fit.2))
```

Yes. The tree is different because it only uses X1 to form the decision tree.

## Problem 2
```{r}
#load data set
iq_data <- read.csv("IQ.Full.csv")
```

```{r}
set.seed(10)

#take subset of 100 subjects
sample100 <- iq_data[sample(1:nrow(iq_data), 100,
  	replace=FALSE),c("AFQT","Coding","Auto","Mechanic","Elec","Science","Math", "Arith", "Word", "Parag", "Numer")]

#sample100.scale <- scale(sample100, center=TRUE, scale=TRUE)

pc.asvab <- prcomp(sample100, scale=TRUE)

pc1.loadings <- data.frame(pc.asvab$rotation[,"PC1"])
pc2.loadings <- data.frame(pc.asvab$rotation[,"PC2"])
```
### 1)
#### a) 
PC1 and PC2 are uncorrelated and are both unit vectors. The loadings for PC1 and PC2 are
```{r}
pc1.loadings
pc2.loadings
```

#### b) 
The PC1 score is obtained by substituting the PC1 loadings and each person's ASVAB scores into the following equation. $$Z_1 = \phi_{11}*X_1 + \phi_{12}*X_2 +  ... \phi_{101}*X_10 + \phi_{111}*X_11$$

 More concretely, it is Z1 = `r pc1.loadings[,1][1]` * X1 + 
 `r pc1.loadings[,1][2]` * X2 + 
 `r pc1.loadings[,1][3]` * X3 + 
 `r pc1.loadings[,1][4]` * X4 + 
 `r pc1.loadings[,1][5]` * X5 + 
 `r pc1.loadings[,1][6]` * X6 + 
 `r pc1.loadings[,1][7]` * X7 + 
 `r pc1.loadings[,1][8]` * X8 + 
 `r pc1.loadings[,1][9]` * X9 + 
 `r pc1.loadings[,1][10]` * X10 + 
 `r pc1.loadings[,1][11]` * X11
 
#### c) 
The scores are uncorrelated.

#### d) 

```{r}
pve.asvab <- 100* (pc.asvab$sdev)^2/sum ((pc.asvab$sdev)^2)
plot(pve.asvab, pch=16, 
     xlab="Principal Components",
     ylab="Prop. of variance explained",
     main="PVE ASVAB plot")
```
This graph shows the proportion of the total variance that each principal component explains. The first component explains `r pve.asvab[1]`% of the total variance, the second PC component explains `r pve.asvab[2]`% of the total variance and so on.

#### e)
```{r}
cpve.asvab <- 100*cumsum((pc.asvab$sdev)^2)/11   

# Scree plot of CPVE's
plot(seq(1:11), cpve.asvab, pch=16, ylim=c(0, 100),
     main="Cumulative Proportion of Variance Explained",
     xlab="Number of PC's Used")

first2pcvariance <- cpve.asvab[2]
```
The first two components explain `r first2pcvariance`% of the total variance.

#### f)
```{r}
plot(pc.asvab$x[, 1], pc.asvab$x[, 2],
     xlim=c(-10, 10), ylim=c(-10, 10),
     xlab="PC1", ylab="PC2",
     main = "Biplot of the 2 main PC's")
abline(v=0, h=0)
```
Here we plot PC1 as the x-axis and PC2 as the y-axis. We can see that the graph has an overall flattened oval shape which closely matches what we expect. PC1 has higher variability and so we expect its "spread" to be greater than PC2's. 
TODO: need to explain what each PC is/contains

#### g)
```{r}
plot(pc.asvab$x[, 1], pc.asvab$x[, 2], col=iq_data$Gender,
     xlim=c(-10, 10), ylim=c(-10, 10),
     xlab="PC1", ylab="PC2")
abline(v=0, h=0)
legend("bottomright", legend=c(as.character(levels(iq_data$Gender))),
       lty=c(1,1), lwd=c(2,2), col=iq_data$Gender)
```

There seems to be no systematic separation between Female and Male using just these two principal components.
TODO: needs to be elaborated, specifically what PCS each mean.

### 2)
#### a)
```{r}
set.seed(10)

#take subset of 100 subjects
esteem100 <- iq_data[sample(1:nrow(iq_data), 100,
  	replace=FALSE),c("Esteem1", "Esteem2", "Esteem3", "Esteem4", "Esteem5", "Esteem6", "Esteem7", "Esteem8", "Esteem9", "Esteem10")]

#reverse esteem scores
esteem100[,  c(1, 2, 4, 6, 7)]  <- 5- esteem100[,  c(1, 2, 4, 6, 7)]
```

#### b)
```{r}
#esteem100.scale <- scale(esteem100, center=TRUE, scale=TRUE)
pc.esteem <- prcomp(esteem100, scale=TRUE)
pc1.esteem.loadings <- data.frame(pc.esteem$rotation[,"PC1"])
```
The loadings for PC1 are
```{r}
pc1.esteem.loadings
```

#### c)
```{r}
# PVE plot
pve.esteem <- 100* (pc.esteem$sdev)^2/sum ((pc.esteem$sdev)^2)
plot(pve.esteem, pch=16, 
     xlab="Principal Components",
     ylab="Prop. of variance explained",
     main="PVE esteem plot")

cpve.esteem <- 100*cumsum((pc.esteem$sdev)^2)/10   

# Scree plot of CPVE's
plot(seq(1:10), cpve.esteem, pch=16, ylim=c(0, 100),
     main="Cumulative Proportion of Variance Explained",
     xlab="Number of PC's Used")

```
PC1 explains `r pve.esteem[1]`% of the total variance.

#### d)
```{r}
plot(pc.esteem$x[, 1], pc.esteem$x[, 2],
     xlim=c(-10, 10), ylim=c(-10, 10),
     xlab="PC1", ylab="PC2",
     main = "Biplot of the 2 main esteem PC's")
abline(v=0, h=0)
```
TODO: brief summary of esteem scores

### 3)
#### a)
It is important to use the logarithmic transformation for income because a unit increase at high amounts of income means less than a unit increase at a low income (poor student vs wealthy professor example from class where 10$ means more to the student than professor). Adjusting it to a logarithmic scale adjusts for the nature appropriately.

#### b)
```{r}
pc.asvab.all <- prcomp(iq_data[,c("AFQT","Coding","Auto","Mechanic","Elec","Science","Math", "Arith", "Word", "Parag", "Numer")], scale=TRUE)
```

#### c)
```{r}
logIncome <- log(iq_data$Income2005)
data.b.pc <- data.frame(logIncome, pc.asvab.all$x)
fit1 <- lm(logIncome~PC1, data.b.pc)
fit2 <- lm(logIncome~PC1+PC2+PC3+PC4+PC5, data.b.pc)
summary(fit1)
summary(fit2)

#Anova(fit1)
#Anova(fit2)

cpve.asvab.all <- 100*cumsum((pc.asvab.all$sdev)^2)/11   

# Scree plot of CPVE's
plot(seq(1:11), cpve.asvab.all, pch=16, ylim=c(0, 100),
     main="Cumulative Proportion of Variance Explained",
     xlab="Number of PC's Used")
```
TODO: why is the LS estimate for fit1 and fit2 identical for PC1??
Using the elbow rule on the CPVE plot, we take the first 5 leading PCs. The 5 leading PCs are significant at the 0.05 level for predicting Income. However, PC1, PC2, PC4 and PC5 are significant at the 0.001 level.

#### d)
```{r}
data.b.pc.control <- data.frame(logIncome, pc.asvab.all$x, iq_data[,c("Race","Gender","Educ")], iq_data[,c("Imagazine","Inewspaper","Ilibrary","MotherEd","FatherEd")])

fit3 <- lm(logIncome~PC1+PC2+PC3+PC4+PC5+Imagazine+Inewspaper+Ilibrary+MotherEd+FatherEd, data.b.pc.control)

Anova(fit3)
```
Yes, controlling for personal demographic variables and household environment variables, the leading 5 PCs are significant at the 0.01 level for predicting income. TODO: need to explain what each of the PCs are

## Problem 3
### 1)
```{r}
yelp.data <- read.csv("yelp_subset.csv", as.is=TRUE)

set.seed(1)

# random sample of 20,000 reviews
yelp.sample <- yelp.data[sample(1:nrow(yelp.data), 20000,
  	replace=FALSE),]

# 1. Make corpus which is collection of texts
# VCorpus (Volatile Corpus) 
mycorpus1 <- VCorpus( VectorSource(yelp.sample$text))

# 2, Change to lower case
mycorpus2 <- tm_map(mycorpus1, content_transformer(tolower))

# 3, Remove some non-content words 
mycorpus3<- tm_map(mycorpus2, removeWords, stopwords("english"))

# 4, Remove punctuations
mycorpus4 <- tm_map(mycorpus3, removePunctuation)

# 5, Remove numbers 
mycorpus5 <- tm_map(mycorpus4, removeNumbers)

# 6, Stem words
mycorpus6 <- tm_map(mycorpus5, stemDocument, lazy = TRUE)

#added extra step due to tm incompatibility for 0.7-1
mycorpus7 <- Corpus(VectorSource(mycorpus6))

# 7, Ready to get word frequency matrix
dtm1 <- DocumentTermMatrix(mycorpus7)   ## library = collection of words for all documents

# Cut the bag to only include the words appearing at least 2% of the time
threshold <- .02*length(mycorpus6)   # 2% of the total documents 
words.10 <- findFreqTerms(dtm1, lowfreq=threshold)  # words appearing at least among 2% of the documents

dtm.10<- DocumentTermMatrix(mycorpus7, control=list(dictionary = words.10))
```
#### i)
This matrix records the frequency of words per document. The 100th row represents the 100th document. The 405th column represents the 405th word. Together, the 100,405 tells you the frequency with which the 405th word appears in the 100th document.
TODO: right now DTM reduces to 4 documents so there's no way of getting 100,405, have to keep tabs on piazza to see what's up

#### ii)
The sparsity of the dtm is 75%. This means that the matrix is quite sparse and that there are many 0's across all entries in the matrix. This makes sense as every document only contains a small set of words from the whole bag of words.

### 2)
```{r}
# Set stars as a categorical variable. Also group stars 1-3 as 0 and 4,5 as 1
yelp.sample$rating <- c(0)
yelp.sample$rating[yelp.sample$stars >= 4] <- 1
yelp.sample$rating <- as.factor(yelp.sample$rating)

#combine dtm and rating
data2 <- data.frame(yelp.sample,as.matrix(dtm.10) )  

#get training and testing split
train <- sample(1:nrow(yelp.sample), 15000, replace=FALSE)
yelp.train <- data2[train,]
yelp.test <- data2[-(train),]
```