---
title: "Modern Data Mining - HW 2"
author:
- Aditi Jayashankar
- Eddie Kong
- Sahana Vijaya Prasad
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=5, fig.width=11, warning = F)
if(!require('pacman')) {
  install.packages('pacman')
}
pacman::p_load(glmnet, leaps, car, tidyverse, mapproj)

# constants for homework assignments
hw_num <- 2
hw_due_date <- "10 October, 2017"
```

## Overview / Instructions

This is homework #`r paste(hw_num)` of STAT 471/571/701. It will be **due on `r paste(hw_due_date)` by 11:59 PM** on Canvas. You can directly edit this file to add your answers. Submit the Rmd file, a PDF or word or HTML version with only 1 submission per HW team.


Solutions will be posted. Make sure to go through these files to pick up some tips.

```{r library, include=FALSE}
# add your library imports here:
library(dplyr)
library(leaps)
library(ggplot2)
library(glmnet)
library(reshape2)
```

## Problem 0

Review the code and concepts covered during lecture: model selection and penalized regression through elastic net. 

## Problem 1: Model Selection

Do ISLR, page 262, problem 8, and write up the answer here. This question is designed to help understanding of model selection through simulations. 

#### (a)
```{r}
set.seed(19)
n <- 100
x <- rnorm(n)
epsilon <- rnorm(n)
```

#### (b)
The betas are beta0=0.25, beta1=1.9, beta2=1.23, beta3=0.89
```{r}
Y <- 0.25 + 1.9 * x + 1.23 * x^2 + 0.89 * x^3 + epsilon
```

#### (c)
```{r}
df <- data.frame(Y, x, x^2, x^3, x^4, x^5, x^6, x^7, x^8, x^9, x^10)
models <- regsubsets(Y ~ ., df, nvmax = 10, method='exhaustive')

summ <- summary(models)

best_cp <- which.min(summ$cp)
best_bic <- which.min(summ$bic)
best_adjr2 <- which.max(summ$adjr2)
```
The best model according to BIC is using `r best_bic` predictors, according to CP using `r best_cp` predictors and using Adjusted R^2 `r best_adjr2` predictors.

```{r}
par(mfrow=c(3, 1), mar=c(2.5, 4, 0.5, 1), mgp=c(1.5, 0.5, 0))
plot(summ$cp, xlab="Number of predictors", 
     ylab="cp", col="red", type="p", pch=16)
plot(summ$bic, xlab="Number of predictors", 
     ylab="bic", col="blue", type="p", pch=16)
plot(summ$adjr2, xlab="Number of predictors", 
     ylab="adjr2", col="green", type="p", pch=16)
```

The coefficients of the best model (using 3 predictors) are:
```{r}
coef(models, id = 3)
```

#### (d)
```{r}
models.f <- regsubsets(Y ~ ., df, nvmax = 10, method='forward')

summ.f <- summary(models.f)

best_cp.f <- which.min(summ.f$cp)
best_bic.f <- which.min(summ.f$bic)
best_adjr2.f <- which.max(summ.f$adjr2)
```
The best model according to BIC is using `r best_bic.f` predictors, according to CP using `r best_cp.f` predictors and using Adjusted R^2 `r best_adjr2.f` predictors.

```{r}
par(mfrow=c(3, 1), mar=c(2.5, 4, 0.5, 1), mgp=c(1.5, 0.5, 0))
plot(summ.f$cp, xlab="Number of predictors", 
     ylab="cp", col="red", type="p", pch=16)
plot(summ.f$bic, xlab="Number of predictors", 
     ylab="bic", col="blue", type="p", pch=16)
plot(summ.f$adjr2, xlab="Number of predictors", 
     ylab="adjr2", col="green", type="p", pch=16)
```
The coefficients of the best model (using 3 predictors) through forward selection are:
```{r}
coef(models.f, id = 3)
```

```{r}
models.b <- regsubsets(Y ~ ., df, nvmax = 10, method='backward')

summ.b <- summary(models.b)

best_cp.b <- which.min(summ.b$cp)
best_bic.b <- which.min(summ.b$bic)
best_adjr2.b <- which.max(summ.b$adjr2)
```
The best model according to BIC is using `r best_bic.b` predictors, according to CP using `r best_cp.b` predictors and using Adjusted R^2 `r best_adjr2.b` predictors.

```{r}
par(mfrow=c(3, 1), mar=c(2.5, 4, 0.5, 1), mgp=c(1.5, 0.5, 0))
plot(summ.b$cp, xlab="Number of predictors", 
     ylab="cp", col="red", type="p", pch=16)
plot(summ.b$bic, xlab="Number of predictors", 
     ylab="bic", col="blue", type="p", pch=16)
plot(summ.b$adjr2, xlab="Number of predictors", 
     ylab="adjr2", col="green", type="p", pch=16)
```
The coefficients of the best model (using 3 predictors) through backward selection are:
```{r}
coef(models.b, id = 3)
```

The results are the same for forward and backward selection for this particular choice of betas and seed. Although, the results may vary between forward and backward selection because forward starts with 0 predictors and grows and backward starts with all predictors and shrinks.

#### (e)
Without CV
```{r}
X.lasso <- matrix(data=c(x, x^2, x^3, x^4, x^5, x^6, x^7, x^8, x^9, x^10),nrow=100)
models.lasso <- glmnet(X.lasso, Y, alpha = 1)

plot(models.lasso)
```
With CV
```{r}
models.lasso.cv <- cv.glmnet(X.lasso, Y, alpha = 1)

plot(models.lasso.cv)
```

```{r}
plot(models.lasso.cv$lambda, models.lasso.cv$cvm, xlab=expression(lambda), ylab="Mean cv error", col="red", pch=16)
```
The LASSO coefficients using lambda.min = `r models.lasso.cv$lambda.min` are:
```{r}
coef(models.lasso.cv, s = "lambda.min")
```
The LASSO coefficients using lambda.1se = `r models.lasso.cv$lambda.1se` are:
```{r}
coef(models.lasso.cv, s = "lambda.1se")
```
We chose lambda.min to fit the model with the following coefficients:
```{r}
fit.min.lm <- lm(Y ~ x+x.2+x.3, data=df)
coef(fit.min.lm) # output lm estimates
```

#### (f)
```{r}
Y.new = 0.3 + 7 * x^7 + epsilon
df.new <- data.frame(Y.new, x, x^2, x^3, x^4, x^5, x^6, x^7, x^8, x^9, x^10)
models.new <- regsubsets(Y.new ~ ., df.new, nvmax = 10, method='exhaustive')

summ.new <- summary(models.new)

best_cp.new <- which.min(summ.new$cp)
best_bic.new <- which.min(summ.new$bic)
best_adjr2.ew <- which.max(summ.new$adjr2)
```
The best model according to BIC is using `r best_bic.new` predictors, according to CP using `r best_cp.new` predictors and using Adjusted R^2 `r best_adjr2.new` predictors.

```{r}
par(mfrow=c(3, 1), mar=c(2.5, 4, 0.5, 1), mgp=c(1.5, 0.5, 0))
plot(summ.new$cp, xlab="Number of predictors", 
     ylab="cp", col="red", type="p", pch=16)
plot(summ.new$bic, xlab="Number of predictors", 
     ylab="bic", col="blue", type="p", pch=16)
plot(summ.new$adjr2, xlab="Number of predictors", 
     ylab="adjr2", col="green", type="p", pch=16)
```

The coefficients of the best model (using 1 predictor) are:
```{r}
coef(models.new, id = 1)
```

LASSO Model
```{r}
X.lasso.new <- matrix(data=c(x, x^2, x^3, x^4, x^5, x^6, x^7, x^8, x^9, x^10),nrow=100)
models.lasso.new <- cv.glmnet(X.lasso.new, Y.new, alpha = 1)
plot(models.lasso.new)
```

```{r}
plot(models.lasso.new$lambda, models.lasso.new$cvm, xlab=expression(lambda), ylab="Mean cv error", col="red", pch=16)
```
The LASSO coefficients using lambda.min = `r models.lasso.new$lambda.min` are:
```{r}
coef(models.lasso.new, s = "lambda.min")
```
The LASSO coefficients using lambda.1se = `r models.lasso.new$lambda.1se` are:
```{r}
coef(models.lasso.new, s = "lambda.1se")
```
We chose lambda.min to fit the model with the following coefficients:
```{r}
fit.min.lm.new <- lm(Y.new ~ x.7, data=df)
coef(fit.min.lm.new) # output lm estimates
```

## Problem 2: Regularization

Crime data continuation:  We use a subset of the crime data discussed in class, but only look at Florida and California. `crimedata` is available on Canvas; we show the code to clean here. 

```{r}
crime <- read.csv("CrimeData.csv", stringsAsFactors = F, na.strings = c("?"))
crime <- dplyr::filter(crime, state %in% c("FL", "CA"))
```

Our goal is to find the factors which relate to violent crime. This variable is included in crime as `crime$violentcrimes.perpop`.

**A)** EDA

* Clean the data first
* Prepare a set of sensible factors/variables that you may use to build a model
* Show the heatmap with mean violent crime by state. You may also show a couple of your favorite summary statistics by state through the heatmaps.
* Write a brief summary based on your EDA

```{r, results='hide'}
#Checking NAs
na_count <- sapply(crime, function(y) sum(length(which(is.na(y)))))
data.frame(na_count)
```

```{r, results='hide'}
#Removing NAs
crime2 <- crime[, -which(colMeans(is.na(crime)) > 0.5)]

na_count <- sapply(crime2, function(y) sum(length(which(is.na(y)))))
data.frame(na_count)
```

```{r, results='hide'}
#Only keep violent crimes
crime.final <- crime2[-c(106:121, 123)]
names(crime.final)
```

```{r, include=F}
#removed state, fold, community, num.urban, other.percap, num.underpov, num.vacant.house, pct.police.drugunits. same cleaning treatment as class example of cleandata.csv
crime.final.noCat <- crime.final[-c(307),-c(1:3, 14, 29, 31, 75, 105)]
```


```{r, echo=F}
#Create Heatmap of USA crimerates
data.s <- crime.final %>%
  group_by(state) %>%
  summarise(
    mean.income=mean(med.income), 
    mean.unemp=mean(pct.unemployed),
    crime.rate=mean(violentcrimes.perpop, na.rm=TRUE), #ignore the missing values
    n=n())

#Create a new data frame with mean income and corresponding state name
crimerate <- data.s[, c("state", "crime.rate")]

#Change abbreviations to names. eg: PA --> Pennsylvania, CA --> California
crimerate$region <- tolower(state.name[match(crimerate$state, state.abb)])

#Add the center coordinate for each state `state.center` contains the coordinate corresponding to `state.abb` in order.
crimerate$center_lat  <- state.center$x[match(crimerate$state, state.abb)]
crimerate$center_long <- state.center$y[match(crimerate$state, state.abb)]

#Load US map info
states <- map_data("state") 

#Combine the US map data with the crimerate data
map <- merge(states, crimerate, sort=FALSE, by="region", all.x=TRUE)

#Re-establish the point order
map <- map[order(map$order),]

ggplot(map, aes(x=long, y=lat, group=group))+
  geom_polygon(aes(fill=crime.rate))+
  geom_path()+ 
  geom_text(data=crimerate, aes(x=center_lat, y=center_long, group=NA, 
                             label=state, size=2), show.legend =FALSE)+
  scale_fill_continuous(limits=c(0, 2000),name="Mean Crime Rate",
                        low="light blue", high="dark blue")   # you may play the colors here
```
```{r, echo=F}
#Mean income heatmap
income <- data.s[, c("state", "mean.income")]
#Change abbreviations to names. eg: PA --> Pennsylvania, CA --> California
income$region <- tolower(state.name[match(income$state, state.abb)])

#Add the center coordinate for each state `state.center` contains the coordinate corresponding to `state.abb` in order.
income$center_lat  <- state.center$x[match(income$state, state.abb)]
income$center_long <- state.center$y[match(income$state, state.abb)]

#Combine the US map data with the income data
map2 <- merge(states, income, sort=FALSE, by="region", all.x=TRUE)

#Re-establish the point order
map2 <- map2[order(map2$order),]

ggplot(map2, aes(x=long, y=lat, group=group))+
  geom_polygon(aes(fill=mean.income))+
  geom_path()+ 
  geom_text(data=income, aes(x=center_lat, y=center_long, group=NA, 
                             label=state, size=2), show.legend =FALSE)+
  scale_fill_continuous(limits=c(1000, 60000),name="Mean Income",
                        low="light blue", high="dark blue")   # you may play the colors here
```
```{r, echo=F}
# Mean unemployed percentage heatmap
unemp <- data.s[, c("state", "mean.unemp")]

#Change abbreviations to names. eg: PA --> Pennsylvania, CA --> California
unemp$region <- tolower(state.name[match(unemp$state, state.abb)])

#Add the center coordinate for each state `state.center` contains the coordinate corresponding to `state.abb` in order.
unemp$center_lat  <- state.center$x[match(unemp$state, state.abb)]
unemp$center_long <- state.center$y[match(unemp$state, state.abb)]

#Combine the US map data with the unemployment data
map3 <- merge(states, unemp, sort=FALSE, by="region", all.x=TRUE)

#Re-establish the point order
map3 <- map3[order(map3$order),]

ggplot(map3, aes(x=long, y=lat, group=group))+
  geom_polygon(aes(fill=mean.unemp))+
  geom_path()+ 
  geom_text(data=unemp, aes(x=center_lat, y=center_long, group=NA, 
                             label=state, size=2), show.legend =FALSE)+
  scale_fill_continuous(limits=c(0, 10),name="Mean Unemployment Percentage",
                        low="light blue", high="dark blue")   # you may play the colors here
```

```{r, include=F}
summary(crime.final)
```

A quick summary: We've excluded all states except CA and FL while removing NA's and non violent crimes from the data set. Florida seems to have a higher crime rate and a lower mean income. However, percent unemployment is lower. We'll have to perform additional analysis before arriving at any conclusion. 


**B)** Use LASSO to choose a reasonable, small model. Fit an OLS model with the variables obtained. The final model should only include variables with p-values < 0.05. Note: you may choose to use lambda 1se or lambda min to answer the following questions where apply. 

1. What is the model reported by LASSO? 

```{r}
Y.crime <- crime.final.noCat[, 98]
X.crime <- model.matrix(violentcrimes.perpop~., data=crime.final.noCat)
crimes.lasso.cv <- cv.glmnet(X.crime, Y.crime, alpha = 1, nfolds=10)
plot(crimes.lasso.cv)
coef.min <- coef(crimes.lasso.cv, s="lambda.1se")  #s=c("lambda.1se","lambda.min") or lambda value
coef.min <- coef.min[which(coef.min !=0),]   # get the non=zero coefficients
coef.min  # the set of predictors chosen
lasso.vars = rownames(as.matrix(coef.min)) # shows only names, not estimates  
```

We choose lambda1se as our lambda after performing kfold cross validation to obtain a smaller set of variables than with lambdamin and arrive at a simpler model.  This is because we believe that many of these variables influence each other (beyond the scope of our assumption that variables are independent - many socioeconomic factors have a strong correlation in real life). It is best to arrive at a smallest as possible but still relevant set of variables to examine.

The model is `r coef.min`
$$Y = 1810.355977 + 7.952891(race.pctblack) -18.222789(pct.kids2parents) + 78.476883(pct.kids.nvrmarried)$$

2. What is the model after running OLS?

```{r}
var.min <- rownames(as.matrix(coef.min)) # output the names
lm.input <- as.formula(paste("violentcrimes.perpop", "~", paste(var.min[-1], collapse = "+"))) # prepare for lm fomulae
lm.input

fit.min.lm <- lm(lm.input, data=crime.final.noCat)
lm.output <- coef(fit.min.lm) # output lm estimates
summary(fit.min.lm) 
```
The OLS model is $$Y = 2012.949 + 13.956(race.pctblack) -22.678(pct.kids2parents) + 94.953(pct.kids.nvrmarried)$$

3. What is your final model, after excluding high p-value variables? You will need to use model selection method to obtain this final model. Make it clear what criterion/criteria you have used and justify why they are appropriate. 

Upon closer examination of the variables we are using, we notice that the p-values of all variables is < 0.05. This is most likely due to the fact that we've used lambda1se. Had we used lambdamin or another variable, the model is slightly more complex (it includes variables such as pct.house.vacant and num.in.shelters) of which some are above 0.05 and we would exclude. Our final model therefore remains the same as above and is:
$$Y = 2012.949 + 13.956(race.pctblack) -22.678(pct.kids2parents) + 94.953(pct.kids.nvrmarried)$$


**C)** Now, instead of Lasso, we want to consider how changing the value of alpha (i.e. mixing between Lasso and Ridge) will affect the model. Cross-validate between alpha and lambda, instead of just lambda. Note that the final model may have variables with p-values higher than 0.05; this is because we are optimizing for accuracy rather than parsimoniousness. 

1. What is your final elastic net model? What were the alpha and lambda values? What is the prediction error?

2. Use the elastic net variables in an OLS model. What is the equation, and what is the prediction error.

3. Summarize your findings, with particular focus on the difference between the two equations.
 

**B+)** Repeat similar stepts as that of **B)** but start with the set of variables that also include all two way interactions

1. How many variables do you have now?

2. Comparing the final models with the ones from **B)**, which one would you use? Commenting on your choice.

