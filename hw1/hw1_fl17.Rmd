---
title: "STAT 471/571/701 Modern Data Mining - HW 1"
author:
- Aditi Jayashankar
- Eddie Kong
- Sahana Vijaya Prasad
date: 'Due: September 17, 2017'
output:
  html_document: default
  pdf_document: default
  word_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=5, fig.width=11, warning = F)
if (!require("pacman")){
  install.packages("pacman")
}

pacman::p_load(ggplot, ggthemes, dplyr, data.table)
library(ggplot2)
# constants for homework assignments
hw_num <- 1
hw_due_date <- "September 17, 2017"

```
# EDA

## Question 1: Exploratory Data Analysis with Sirius XM

### Q1.1

Load the data into R. 

```{r}
radio <- read.csv("Survey_results_final.csv", header = TRUE,
                  stringsAsFactors = F, na.strings = c("", "NA", "select one", "NA's"))
```

For each of the following 2 questions, there is a `dplyr` solution and a `base` R solution. Provide *both* ways of doing so. 

i. We need to clean and select only the variables of interest. Select only the variables Age, Gender, Education Level, Household Income in 2013, Sirius Listener?, Wharton Listener? and Time used to finish the survey.

```{r}
#baseR method

variablesToInclude <- c("Answer.Age", "Answer.Gender", "Answer.Education", 
                        "Answer.HouseHoldIncome","Answer.Sirius.Radio", 
                        "Answer.Wharton.Radio", "WorkTimeInSeconds")
radio2 <- copy(radio[variablesToInclude])
radio2 <- radio2[variablesToInclude]

#dplyr method
radio <-radio %>%
  select(Answer.Age, Answer.Gender, Answer.Education, Answer.HouseHoldIncome,
         Answer.Sirius.Radio, Answer.Wharton.Radio, WorkTimeInSeconds)

```

ii. Change the variable names to be "age", "gender", "education", "income", "sirius", "wharton", "worktime".

```{r}
#baseR method
variablesToInclude <- c("Answer.Age", "Answer.Gender", "Answer.Education", 
                        "Answer.HouseHoldIncome","Answer.Sirius.Radio", 
                        "Answer.Wharton.Radio", "WorkTimeInSeconds")

names(radio2)[names(radio2) == "Answer.Age"] <- "age"
names(radio2)[names(radio2) == "Answer.Gender"] <- "gender"
names(radio2)[names(radio2) == "Answer.Education"] <- "education"
names(radio2)[names(radio2) == "Answer.HouseHoldIncome"] <- "income"
names(radio2)[names(radio2) == "Answer.Sirius.Radio"] <- "sirius"
names(radio2)[names(radio2) == "Answer.Wharton.Radio"] <- "wharton"
names(radio2)[names(radio2) == "WorkTimeInSeconds"] <- "worktime"

# dplyr method
radio <- radio %>%
  rename(age = Answer.Age, gender = Answer.Gender, education = Answer.Education,
         income = Answer.HouseHoldIncome, sirius = Answer.Sirius.Radio, wharton = 
           Answer.Wharton.Radio, worktime = WorkTimeInSeconds)

```

### Q1.2

Some flaws we've identified in the data with View and through dplyr queries, namely distinct and filter: 

* missing age or incorrectly formatted ("female, Eighteen") or invalid ("223")
  * for missing age
  * for incorrectly formatted values
  * for invalid values
* missing education ("select one option")
  * for missing education
* missing household income
* missing sirius response
* missing wharton response

Ignore all invalid responses.
  
```{r}
#convert from age factor to numeric
radio["age"] <- lapply(radio["age"], function(x) as.numeric(as.character(x)))
#convert to factor
radio["education"] <- lapply(radio["education"], function(x) as.factor(x))
radio["income"] <- lapply(radio["income"], function(x) as.factor(x))
radio["gender"] <- lapply(radio["gender"], function(x) as.factor(x))
radio["sirius"] <- lapply(radio["sirius"], function(x) as.factor(x))
radio["wharton"] <- lapply(radio["wharton"], function(x) as.factor(x))

#remove invalid age rows
radio <- radio %>%
  filter(!is.na("age") & age < 120 & age > 17)

#remove empty fields
radio <- radio %>%
  filter(!is.na("education"))

radio <- radio %>%
  filter(!is.na("income"))

radio <- radio %>%
  filter(!is.na("gender"))

radio <- radio %>%
  filter(!is.na("sirius"))

radio <- radio %>%
  filter(!is.na("wharton"))

radio <- na.omit(radio)
summary(radio)

```
  
These responses could've been invalid because of the options offered by the survey and the fact that there is no input validation for correctness or formatting on the MTurk forms.

### Q1.3

```{r}
radio3 <- copy(radio)

summary(radio3)

radio3 %>%
  select(age) %>%
  ggplot(aes(x = age)) + geom_histogram(stat="count") +
  labs(title="Age Distribution of Responders")

radio3 %>%
  select(education) %>%
  ggplot(aes(x = education)) + geom_histogram(stat="count") +
  labs(title="Education Distribution of Responders")

radio3 %>%
  select(income) %>%
  ggplot(aes(x = income)) + geom_histogram(stat="count") +
  labs(title="Income Distribution of Responders")

radio3 %>%
  select(gender) %>%
  ggplot(aes(x = gender)) + geom_histogram(stat="count") +
  labs(title="Gender Distribution of Responders")

radio3 %>%
  select(sirius) %>%
  ggplot(aes(x = sirius)) + geom_histogram(stat="count") +
  labs(title="Sirius Responses")

radio3 %>%
  select(wharton) %>%
  ggplot(aes(x = wharton)) + geom_histogram(stat="count") +
  labs(title="Wharton Responses")
```
Write a brief report to summarize all the variables collected. Include both summary statistics (including sample size) and graphical displays such as histograms or bar charts where appropriate. Comment on what you have found from this sample. (For example - it's very interesting to think about why would one work for a job that pays only 10cents/each survey? Who are those survey workers? The answer may be interesting even if it may not directly relate to our goal.)

* 78% of the respondents earn < $75,000 per year
* Those earning less than $15,000 might be home-makers, retired professionals for extra income
* 59% are in the age range of 18-29 years. They might be young professionals who use a lot of internet and want to earn a little extra income in their spare time

### Q1.4 Sample property questions

i. Does this sample appear to be a random sample from the general population of the USA? 

No, it is not a random sample because:
* age distribution in the sample does not match the general population of USA
* gender distribution does not match either because male-female ratio is approximately 1:1 in USA

Sources:
1. US Census Bureau
2. http://www.kff.org/other/state-indicator/distribution-by-age/
3. http://www.kff.org/other/state-indicator/distribution-by-gender/

ii. Does this sample appear to be a random sample from the MTURK population?

Yes, it a reasonably random sample of the MTURK population:
* age, gender and income are similar in both populations

Sources:
http://www.pewinternet.org/2016/07/11/research-in-the-crowdsourcing-age-a-case-study/pi_2016-07-11_mechanical-turk_4-01/

### Q1.5

4. Give a final estimate of the Wharton audience size in January 2014. Assume that the sample is a random sample of the MTURK population, and that the proportion of Wharton listeners vs. Sirius listeners remains the same in the general population as it is in the MTURK population. Briefly summarize your findings and how you came to that conclusion.

```{r}
numberOfSiriusMTurkListeners <- radio %>%
  filter(sirius == "Yes")

numberOfWhartonMTurkListeners <- radio %>%
  filter(sirius == "Yes" & wharton == "Yes")

ratioOfWhartonToSiriusListeners <- nrow(numberOfWhartonMTurkListeners)/nrow(numberOfSiriusMTurkListeners)

estimatedSiriusPopulation <- 51600000
estimatedWhartonListeners <- (ratioOfWhartonToSiriusListeners * estimatedSiriusPopulation)

estimatedWhartonListeners

```
We first calculate the proportion of wharton listeners in the sirius listener population. We then take that ratio and multiply it by 51.6 M which is the number of sirius listeners that we've estimated from class. The estimated size of the Wharton audience is `r estimatedWhartonListeners`

# Simple Regression
    
## Question 2

This exercise is designed to help you understand the linear model and see everything through simulations.

Presume that $x$ and $y$ are linearly related with a normal error, such that $y = 1 + 1.2x + \epsilon$. The standard deviation of the error is $\sigma = 2$. 

Note: we can create a sample input vector ($n = 40$) for $x$ with the following code:

```{r, eval = F}
x <- seq(0, 1, length = 40)
```


### Q2.1

Create a corresponding output vector for $y$ according to the equation given above. Then, create a scatterplot with $\left(x, y\right)$ pairs. Base R plotting is acceptable, but if you can, attempt to use `ggplot2` to create the plot.

```{r}
set.seed(22)
x <- seq(0, 1, length = 40)
y <- 1 + 1.2*x + rnorm(40, sd=2)

plot(x, y, 
       pch  = 16, 
       cex  = 0.8,
       col  = "blue",
       xlab = "X", 
       ylab = "Y",
       main = "Y = 1 + 1.2X + epsilon")

ggplot(data.frame(x, y), aes(x, y)) + geom_point() + labs(title="Y = 1 + 1.2X + epsilon", x='X', y='Y')

```

### Q2.2

Find the LS estimates of $\beta_0$ and $\beta_1$, using the `lm()` function. 
```{r}
lse <- lm(y ~ x)
lse_summary <- summary(lse)$coefficients
lse_summary
```

### Q2.3 

Overlay the LS estimates onto a copy of the scatterplot you made above.
```{r}
par("bg") 
plot(x, y, 
       pch  = 16, 
       cex  = 0.8,
       col  = "blue",
       xlab = "X", 
       ylab = "Y",
       main = "Y = 1 + 1.2X + epsilon")
abline(lse, col="red", lwd=4) 
```

### Q2.4

What is the 95% confidence interval for $\beta_1$? Does this confidence interval capture the true $\beta_1$?

```{r}
lse_summary
xStandardError = lse_summary[2,2]
beta1 = lse_summary[2,1]
tstar <- qt(0.975, 38) # we need 2.5% at each side so 97.5, df = n-1
upperci = beta1 + tstar * xStandardError
lowerci = beta1 - tstar * xStandardError

results <- cbind(xStandardError, beta1, upperci, lowerci)
results
```
The confidence intervals is `r lowerci` to `r upperci`
Yes, this interval does capture the true $\beta_1 = 1.2$
### Q2.5

What is your RSE for this linear model fit? Is it close to $\sigma = 2$?

```{r}
summary(lse)
```

RSE is 1.817 which is close to $\sigma = 2$

### Q2.6

This part aims to help understand the notion of sampling statistics, confidence intervals. Let's concentrate on estimating the slope only.  

Generate 100 samples of size $n = 40$, and estimate the slope coefficient from each sample. We include some sample code below, which should aim you in setting up the simulation. Note: this code is written clearly but    suboptimally; see the appendix for a more R-like way to do this simulation.
```{r}
x <- seq(0, 1, length = 40) 
n_sim <- 100
b1 <- numeric(n_sim)   # nsim many LS estimates of beta1 (=1.2)
upper_ci <- numeric(n_sim)  # lower bound
lower_ci <- numeric(n_sim)  # upper bound
t_star <- qt(0.975, 38)

# Carry out the simulation
for (i in 1:n_sim){
  y <- 1 + 1.2 * x + rnorm(40, sd = 2)
  lse <- lm(y ~ x)
  lse_out <- summary(lse)$coefficients
  se <- lse_out[2, 2]
  b1[i] <- lse_out[2, 1]
  upper_ci[i] <- b1[i] + t_star * se
  lower_ci[i] <- b1[i] - t_star * se
}
results <- cbind(se, b1, upper_ci, lower_ci)
rm(se, b1, upper_ci, lower_ci, x, n_sim, b1, t_star, lse, lse_out)

```

i. Summarize the LS estimates of $\beta_1$ (in the above, `sim_results$b1`). Does the sampling distribution agree with the theory? 

```{r}
summary(results)
```

From the summary, we can say that about 95% of the simulation estimates fall within the 95% CI we constructed.

ii.  How many times do your 95% confidence intervals cover the true $\beta_1$? Display your confidence intervals graphically.

```{r}
res <- data.frame(results)
res$Index <- seq_len(nrow(res))

l <- length(which((res['upper_ci']) >= 1.2 &  (res['lower_ci']) <= 1.2))

ggplot(res, aes(Index, 1.2)) + geom_point() + geom_segment(data=res,
               aes(x=Index,y=upper_ci,yend=lower_ci,xend=Index)) + labs(title='CI for 100 Samples', x='Sample Number')
```

The true $\beta_1$ is covered `r l` times.


# Multiple Regression

## Question 3:

Auto data from ISLR. The original data contains 408 observations about cars. It has some similarity as the data CARS that we use in our lectures. To get the data, first install the package ISLR. The data Auto should be loaded automatically. We use this case to go through methods learnt so far. 

You can access the necessary data with the following code:

```{r}
# check if you have ISLR package, if not, install it
if(!requireNamespace('ISLR')) install.packages('ISLR') 
auto_data <- ISLR::Auto
```

Get familiar with this dataset first. You can use `?ISLR::Auto` to view a description of the dataset. 

### Q3.1
Explore the data, with particular focus on pairwise plots and summary statistics. Briefly summarize your findings and any peculiarities in the data.
```{r}
summary(auto_data)
pairs(auto_data)

```
Summary: Cars from 1970 - 1982 with a wide range of characteristics

We find that there might be a linear relationship between: 
* displacement vs horsepower
* displacement vs weight
* displacement vs cylinders
* horsepower vs weight
* horsepower vs acceleration
* mpg vs horsepower
* mpg vs weight
* mpg vs displacement

### Q3.2
What effect does time have on MPG?

i. Start with a simple regression of mpg vs. year and report R's `summary` output. Is year a significant variable at the .05 level? State what effect year has on mpg, if any, according to this model. 

```{r}
ggplot(auto_data, aes(year,mpg)) + geom_point(size=5) + labs(title="Year vs MPG", x = "Year", y = "MPG") + geom_smooth(method="lm", se=FALSE)

auto_year_lse = lm(auto_data$mpg ~ auto_data$year)
summary(auto_year_lse)
```
Yes, year is a significant variable at the .05 level since our P value < 0.0001
According to our model, for every increase in year, we get a 1.23004 increase in mpg.

ii. Add horsepower on top of the variable year. Is year still a significant variable at the .05 level? Give a precise interpretation of the year effect found here. 

```{r}
ggplot(auto_data, aes(year,mpg)) + geom_point(size=5) + labs(title="Year vs MPG", x = "Year", y = "MPG") + geom_smooth(method="lm", se=FALSE)

auto_yearhp_lse = lm(auto_data$mpg ~ auto_data$year + auto_data$horsepower)
summary(auto_yearhp_lse)
```
Yes, year is still significant at the 0.5 level since P < 0.0001

According to our model, for every increase in year, holding horsepower constant, we can expect a 0.657268 increase in mpg. 

iii. The two 95% CI's for the coefficient of year differ among i) and ii). How would you explain the difference to a non-statistician?

```{r}
t_star_auto <- qt(0.975, 389)

lse_year_out <- summary(auto_year_lse)$coefficients
se <- lse_year_out[2, 2]
b1 <- lse_year_out[2, 1]
upper_ci1 <- b1 + t_star_auto * se
lower_ci2 <- b1 - t_star_auto * se

year_CI <- cbind(se, b1, upper_ci1, lower_ci2)

lse_yearhp_out <- summary(auto_yearhp_lse)$coefficients
se2 <- lse_yearhp_out[2, 2]
b12 <- lse_yearhp_out[2, 1]
upper_ci3 <- b12 + t_star_auto * se2
lower_ci4 <- b12 - t_star_auto * se2

yearhp_CI <- cbind(se2, b12, upper_ci3, lower_ci4)

year_CI 
yearhp_CI 
```

The two CI's are different because these are derived from two different models. One uses only year while the other uses year+horsepower. These two models yield different beta estimates and hence, different confidence intervals.


iiii. Do a model with interaction by fitting `lm(mpg ~ year * horsepower)`. Is the interaction effect significant at .05 level? Explain the year effect (if any).
```{r}
auto_yeartimeshp_lse = lm(auto_data$mpg ~ auto_data$year * auto_data$horsepower)
summary(auto_yeartimeshp_lse)
```
Yes, the interaction is significant at .05 level. For every increase increase in year, holding horsepower constant, mpg increases by 2.17604 (2.192 - 0.01596)

### Q3.3
Remember that the same variable can play different roles! Take a quick look at the variable `cylinders`, try to use this variable in the following analyses wisely. We all agree that larger number of cylinder will lower mpg. However, we can interpret `cylinders` as either a continuous (numeric) variable or a categorical variable.

i. Fit a model, that treats `cylinders` as a continuous/numeric variable: `lm(mpg ~ horsepower + cylinders, ISLR::Auto)`. Is `cylinders` significant at the 0.01 level? What effect does `cylinders` play in this model?

```{r}
auto_cylinders_lse = lm(auto_data$mpg ~ auto_data$horsepower + auto_data$cylinders)
summary(auto_cylinders_lse)
```
Yes, cylinders is significant at the 0.01 level. For every increase in cylinders, holding horsepower constant, we have a decrease in mpg of 1.91982

ii. Fit a model that treats `cylinders` as a categorical/factor variable:  `lm(mpg ~ horsepower + as.factor(cylinders), ISLR::Auto)`. Is `cylinders` significant at the .01 level? What is the effect of `cylinders` in this model? Use `anova(fit1, fit2)` and `Anova(fit2`)` to help gauge the effect. Explain the difference between `anova()` and `Anova`.

```{r}
auto_cat_cylinders_lse = lm(auto_data$mpg ~ auto_data$horsepower + as.factor(auto_data$cylinders))
summary(auto_cat_cylinders_lse)


anova(auto_cylinders_lse, auto_cat_cylinders_lse)

library('car')
Anova(auto_cat_cylinders_lse)
```

It is better to use Anova(fit2) because in anova(fit1, fit2), fit1 is not a subset of fit2 and cannot be compared since the equations are different - cylinders is numeric in fit1 and categorical in fit2.

Yes, cylinders is significant at the 0.01 level.

********What is the effect of `cylinders` in this model*******

```{r}
anova(lm(auto_data$mpg ~ auto_data$horsepower), auto_cat_cylinders_lse)
```

iii. What are the fundamental differences between treating `cylinders` as a numeric and or a factor models? 

The difference between treating cylinder as numeric and as a factor is that it creates completely diffrent models. When treated as a factor, you get many variables and estimates while numeric variable only has one estimate. In the categorical model, the estimates are derived from the first variable in the factor that is treated as the intercept.
When considering cylinders as a numeric variable, when the number of cylinders double, the corresponding increase in mpg also doubles. But this is not true when cylinders is considered as a factor. When the number of cylinders double, the corresponding increase in mpg would not double.

### Q3.4
Final modelling question: we want to explore the effects of each feature as best as possible. You may explore interactions, feature transformations, higher order terms, or other strategies within reason. The model(s) should be as parsimonious (simple) as possible unless the gain in accuracy is significant from your point of view.
  
i. Describe the final model. Include diagnostic plots with particular focus on the model residuals and diagnoses.
ii. Summarize the effects found.
iii. Predict the mpg of a car that is: built in 1983, in US, red, 180 inches long, 8 cylinders, 350 displacement, 260 as horsepower and weighs 4000 pounds. Give a 95% CI.





## Appendix

This is code that is roughly equivalent to what we provide above in Question 2 (simulations).

```{r, eval = F}
simulate_lm <- function(n) {
  # note: `n` is an input but not used (don't worry about this hack)
  x <- seq(0, 1, length = 40) 
  y <- 1 + 1.2 * x + rnorm(40, sd = 2)
  t_star <- qt(0.975, 38)
  lse <- lm(y ~ x)
  lse_out <- summary(lse)$coefficients
  se <- lse_out[2, 2]
  b1 <- lse_out[2, 1]
  upper_CI = b1 + t_star * se
  lower_CI = b1 - t_star * se
  return(data.frame(se, b1, upper_CI, lower_CI))
}

# this step runs the simulation 100 times, 
# then matrix transposes the result so rows are observations 
sim_results <- data.frame(t(sapply(X = 1:100, FUN = simulate_lm)))
```

